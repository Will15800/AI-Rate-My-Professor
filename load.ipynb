{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env.local')\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "import json\n",
    "# import anthropic\n",
    "import groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing 'rag' index...\n",
      "Creating new 'rag' index...\n",
      "New index dimension: 1536\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "if 'rag' in pc.list_indexes().names():\n",
    "    print(\"Deleting existing 'rag' index...\")\n",
    "    pc.delete_index(\"rag\")\n",
    "\n",
    "print(\"Creating new 'rag' index...\")\n",
    "pc.create_index(\n",
    "    name='rag',\n",
    "    dimension=1536, \n",
    "    metric='cosine',\n",
    "    spec=ServerlessSpec(\n",
    "        cloud='aws',\n",
    "        region='us-east-1'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Verify the new index\n",
    "index = pc.Index(\"rag\")\n",
    "index_stats = index.describe_index_stats()\n",
    "print(f\"New index dimension: {index_stats.dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing review 1/20\n",
      "Processing review 2/20\n",
      "Processing review 3/20\n",
      "Processing review 4/20\n",
      "Processing review 5/20\n",
      "Processing review 6/20\n",
      "Processing review 7/20\n",
      "Processing review 8/20\n",
      "Processing review 9/20\n",
      "Processing review 10/20\n",
      "Processing review 11/20\n",
      "Processing review 12/20\n",
      "Processing review 13/20\n",
      "Processing review 14/20\n",
      "Processing review 15/20\n",
      "Processing review 16/20\n",
      "Processing review 17/20\n",
      "Processing review 18/20\n",
      "Processing review 19/20\n",
      "Processing review 20/20\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from groq import Groq\n",
    "\n",
    "# Check if the API key is set\n",
    "if not os.getenv(\"GROQ_API_KEY\"):\n",
    "    raise ValueError(\"GROQ_API_KEY environment variable is not set\")\n",
    "\n",
    "# Initialize GROQ client\n",
    "groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "def get_embedding(text):\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant that generates embeddings. For the given input, respond with a comma-separated list of exactly 1536 floating-point numbers representing the embedding.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Generate an embedding for the following text: {text}\"}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=3072,\n",
    "            timeout=30  # Set a 30-second timeout\n",
    "        )\n",
    "\n",
    "        # Extract the content from the response\n",
    "        embedding_string = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Attempt to split the response into a list of numbers\n",
    "        embedding = [float(x) for x in embedding_string.split(',') if x.strip()]\n",
    "\n",
    "        # Ensure we have exactly 1536 dimensions\n",
    "        if len(embedding) < 1536:\n",
    "            embedding.extend([0.0] * (1536 - len(embedding)))\n",
    "        elif len(embedding) > 1536:\n",
    "            embedding = embedding[:1536]\n",
    "\n",
    "        return embedding\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing embedding: {e}\")\n",
    "        print(f\"Received string: {embedding_string}\")\n",
    "        return [0.0] * 1536  # Return a zero vector in case of failure\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return [0.0] * 1536  # Return a zero vector in case of failure\n",
    "\n",
    "def get_embedding_with_retry(text, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return get_embedding(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt + 1}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Load your data\n",
    "with open(\"reviews.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Process data and generate embeddings\n",
    "processed_data = []\n",
    "for i, review in enumerate(data[\"reviews\"]):\n",
    "    print(f\"Processing review {i+1}/{len(data['reviews'])}\")\n",
    "    embedding = get_embedding_with_retry(review['review'])\n",
    "    processed_data.append(\n",
    "        {\n",
    "            \"values\": embedding,\n",
    "            \"id\": review[\"professor\"],\n",
    "            \"metadata\": {\n",
    "                \"review\": review[\"review\"],\n",
    "                \"subject\": review[\"subject\"],\n",
    "                \"stars\": review[\"stars\"],\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'values': [0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.9789,\n",
       "  0.0123,\n",
       "  0.0456,\n",
       "  0.0987,\n",
       "  0.1234,\n",
       "  0.1567,\n",
       "  0.189,\n",
       "  0.2345,\n",
       "  0.2678,\n",
       "  0.3011,\n",
       "  0.3456,\n",
       "  0.3789,\n",
       "  0.4123,\n",
       "  0.4567,\n",
       "  0.5012,\n",
       "  0.5345,\n",
       "  0.5678,\n",
       "  0.6123,\n",
       "  0.6456,\n",
       "  0.6789,\n",
       "  0.7234,\n",
       "  0.7567,\n",
       "  0.789,\n",
       "  0.8345,\n",
       "  0.8678,\n",
       "  0.9011,\n",
       "  0.9456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  ...],\n",
       " 'id': 'Dr. Sarah Johnson',\n",
       " 'metadata': {'review': 'Dr. Johnson explains complex concepts clearly. Her enthusiasm for physics is contagious!',\n",
       "  'subject': 'Physics',\n",
       "  'stars': 4}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelListResponse(data=[Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Groq', active=True, context_window=1500, public_apps=None), Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None), Model(id='gemma-7b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None), Model(id='llama-3.1-70b-versatile', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None), Model(id='llama3-70b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama3-8b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama3-groq-70b-8192-tool-use-preview', created=1693721698, object='model', owned_by='Groq', active=True, context_window=8192, public_apps=None), Model(id='llama3-groq-8b-8192-tool-use-preview', created=1693721698, object='model', owned_by='Groq', active=True, context_window=8192, public_apps=None), Model(id='llama-guard-3-8b', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='mixtral-8x7b-32768', created=1693721698, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=1500, public_apps=None)], object='list')\n"
     ]
    }
   ],
   "source": [
    "response = groq_client.models.list()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted count: 20\n",
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'ns1': {'vector_count': 20}},\n",
      " 'total_vector_count': 20}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "index = pc.Index(\"rag\")\n",
    "upsert_response = index.upsert(\n",
    "    vectors=processed_data,\n",
    "    namespace=\"ns1\",\n",
    ")\n",
    "print(f\"Upserted count: {upsert_response['upserted_count']}\")\n",
    "\n",
    "# Wait for a few seconds\n",
    "time.sleep(5)\n",
    "\n",
    "# Check stats again\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'ns1': {'vector_count': 20}},\n",
       " 'total_vector_count': 20}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
